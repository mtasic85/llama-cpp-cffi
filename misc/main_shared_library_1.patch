--- main.original.cpp	2024-07-10 13:44:18.220716924 +0200
+++ main.cpp	2024-07-10 13:46:10.303204961 +0200
@@ -39,6 +39,90 @@
 static std::vector<llama_token> * g_output_tokens;
 static bool is_interacting  = false;
 static bool need_insert_eot = false;
+static FILE *llama_stdout = stdout;
+static FILE *llama_stderr = stderr;
+static int (*llama_fprintf)(FILE*, const char*, ...) = fprintf;
+static int (*llama_fflush)(FILE*) = fflush;
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+void llama_set_stdout(FILE* f);
+void llama_set_stderr(FILE* f);
+void llama_set_fprintf(int (*func)(FILE*, const char*, ...));
+void llama_set_fflush(int (*func)(FILE*));
+const char* llama_get_metadata_as_json(int argc, char ** argv);
+void llama_free_metadata_as_json(void* c_output);
+
+#ifdef __cplusplus
+}
+#endif
+
+void llama_set_stdout(FILE* f) {
+    llama_stdout = f;
+}
+
+void llama_set_stderr(FILE* f) {
+    llama_stderr = f;
+}
+
+void llama_set_fprintf(int (*func)(FILE*, const char*, ...)) {
+    llama_fprintf = func;
+}
+
+void llama_set_fflush(int (*func)(FILE*)) {
+    llama_fflush = func;
+}
+
+const char* llama_get_metadata_as_json(int argc, char ** argv) {
+    gpt_params params;
+    gpt_params_parse(argc, argv, params);
+    llama_model_params model_params = llama_model_params_from_gpt_params(params);
+    llama_model * model = llama_load_model_from_file(params.model.c_str(), model_params);
+ 
+    if (model == NULL) {
+        return NULL;
+    }
+ 
+    llama_context_params ctx_params = llama_context_params_from_gpt_params(params);
+    llama_context * ctx = llama_new_context_with_model(model, ctx_params);
+ 
+    if (ctx == NULL) {
+        llama_free_model(model);
+        return NULL;
+    }
+
+    std::string bos = llama_token_to_piece(ctx, llama_token_bos(model));
+    std::string eos = llama_token_to_piece(ctx, llama_token_eos(model));
+    std::string eot = llama_token_to_piece(ctx, llama_token_eot(model));
+
+    std::string output;
+    output.append("{");
+    output.append("\"bos\"");
+    output.append(": \"");
+    output.append(bos);
+    output.append("\", ");
+    output.append("\"eos\"");
+    output.append(": \"");
+    output.append(eos);
+    output.append("\", ");
+    output.append("\"eot\"");
+    output.append(": \"");
+    output.append(eot);
+    output.append("\"\n");
+    output.append("}");
+
+    char* c_output = new char[output.length() + 1];
+    std::memset(c_output, 0, output.length() + 1);
+    std::strcpy(c_output, output.c_str());
+    llama_free(ctx);
+    llama_free_model(model);
+    return c_output;
+}
+
+ void llama_free_metadata_as_json(void* c_output) { free(c_output); }
+
 
 static bool file_exists(const std::string & path) {
     std::ifstream f(path.c_str());
@@ -65,7 +149,7 @@
 
     const bool success = fs_create_directory_with_parents(params.logdir);
     if (!success) {
-        fprintf(stderr, "%s: warning: failed to create logdir %s, cannot write logfile\n",
+        llama_fprintf(llama_stderr, "%s: warning: failed to create logdir %s, cannot write logfile\n",
                 __func__, params.logdir.c_str());
         return;
     }
@@ -74,7 +158,7 @@
     FILE * logfile = fopen(logfile_path.c_str(), "w");
 
     if (logfile == NULL) {
-        fprintf(stderr, "%s: failed to open logfile %s\n", __func__, logfile_path.c_str());
+        llama_fprintf(llama_stderr, "%s: failed to open logfile %s\n", __func__, logfile_path.c_str());
         return;
     }
 
@@ -127,7 +211,18 @@
     return formatted;
 }
 
-int main(int argc, char ** argv) {
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#ifdef SHARED_LIB
+int llama_cli_main(int argc, char ** argv);
+
+int llama_cli_main(int argc, char ** argv)
+#else
+int main(int argc, char ** argv)
+#endif
+{
     gpt_params params;
     g_params = &params;
 
@@ -524,7 +619,7 @@
 
     struct llama_sampling_context * ctx_sampling = llama_sampling_init(sparams);
     if (!ctx_sampling) {
-        fprintf(stderr, "%s: failed to initialize sampling subsystem\n", __func__);
+        llama_fprintf(llama_stderr, "%s: failed to initialize sampling subsystem\n", __func__);
         exit(1);
     }
 
@@ -561,7 +656,7 @@
                 console::set_display(console::error);
                 printf("<<input too long: skipped %d token%s>>", skipped_tokens, skipped_tokens != 1 ? "s" : "");
                 console::set_display(console::reset);
-                fflush(stdout);
+                llama_fflush(llama_stdout);
             }
 
             if (ga_n == 1) {
@@ -761,7 +856,7 @@
                 const std::string token_str = llama_token_to_piece(ctx, id, params.special);
 
                 // Console/Stream Output
-                fprintf(stdout, "%s", token_str.c_str());
+                llama_fprintf(llama_stdout, "%s", token_str.c_str());
 
                 // Record Displayed Tokens To Log
                 // Note: Generated tokens are created one by one hence this check
@@ -774,7 +869,7 @@
                     output_ss << token_str;
                 }
 
-                fflush(stdout);
+                llama_fflush(llama_stdout);
             }
         }
 
@@ -986,3 +1081,7 @@
 
     return 0;
 }
+
+#ifdef __cplusplus
+}
+#endif
\ No newline at end of file
